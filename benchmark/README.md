# Current best benchmark result with other SOA algotrithms

For example:

MaxEnt 
DIRL

AIRL

Multi-goal/Multi-task
Bayesian IRL
Energy-based
State Marginal Matching
Incorporating Kinematics/Enviornment
GAN:Generative adversarial IRL
Scaleble/Meta IRL
Conditional chioice/context
Variational/Auto-encoder 
Mutual Information/ Gaussian Process
Supervised/Semi-supervised
data-based: summary/missing data / human data
Offline IRL
Off-policy AIRL

Behaviour modeling
Relational Event model
Multi-agent/ Group messaging/interaction
(Neural) Architecture search

Robust rewards

Goal-directed 
Visual/ Human attention
Suboptimal Demonstrations

Risk-aware (management)/ Risk-sensitive

Continuous domains

Cooperative IRL

Structual Evolution
Neuroevolution-Based Inverse Reinforcement Learning

Probabilistic Performance Bound

Guided Policy
Explicit policy estimate

Correctness and sample complexity

Exploration Beyond Boundary

State representation learning

Feature construction

Cross-Embodiment

Bellman Gradient

Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal Modeling

Equilibrium Inverse Reinforcement Learning

Identifying Cognitive Radars -- Inverse Reinforcement Learning using Revealed Preferences
(Stochastic optimization)

Human-Interactive Subgoal Supervision for Efficient Inverse Reinforcement Learning

Towards Inverse Reinforcement Learning for Limit Order Book Dynamics


Stochastic
Regularized
Bounded

Repeated Inverse Reinforcement Learning
Lifelong Inverse Reinforcement Learning

Encoding Finite Automata
Sample Complexty
Time-invariant

Non-cooperative
Compatible
Interaction-limited
Function approximation
High-dimensional

Structured classification

Exploring Hierarchy-Aware Inverse Reinforcement Learning


General-Purpose Planning

Online Observer-based

Noisy observations

Resolving Unidentifiability
